{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dialogflow_v2 as dialogflow\n",
    "import os\n",
    "\n",
    "credentials_file = 'newagent-fpxjnq-08c3138e5a20.json'  \n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = credentials_file\n",
    "\n",
    "from google.oauth2 import service_account\n",
    "credentials = service_account.Credentials.from_service_account_file(credentials_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_intent(project_id, display_name, training_phrases_parts, message_texts):\n",
    "    \"\"\"Create an intent of the given intent type.\"\"\"\n",
    "    #import dialogflow_v2 as dialogflow\n",
    "    intents_client = dialogflow.IntentsClient(credentials=credentials)\n",
    "\n",
    "    parent = intents_client.project_agent_path(project_id)\n",
    "    training_phrases = []\n",
    "    for training_phrases_part in training_phrases_parts:\n",
    "        part = dialogflow.types.Intent.TrainingPhrase.Part(\n",
    "            text=training_phrases_part)\n",
    "        # Here we create a new training phrase for each provided part.\n",
    "        training_phrase = dialogflow.types.Intent.TrainingPhrase(parts=[part])\n",
    "        training_phrases.append(training_phrase)\n",
    "\n",
    "    text = dialogflow.types.Intent.Message.Text(text=message_texts)\n",
    "    message = dialogflow.types.Intent.Message(text=text)\n",
    "\n",
    "    intent = dialogflow.types.Intent(\n",
    "        display_name=display_name,\n",
    "        training_phrases=training_phrases,\n",
    "        messages=[message])\n",
    "\n",
    "    response = intents_client.create_intent(parent, intent)\n",
    "\n",
    "    print('Intent created: {}'.format(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_id = 'newagent-fpxjnq'\n",
    "language_code = ''\n",
    "display_name = list(['delete', 'transfer', 'alarm'])\n",
    "training_phrases_parts = list([['delete the row please.', 'delete the column please.'],\n",
    "                            ['transfer 50 euro please', 'i want to make transfer', 'transfer money'],\n",
    "                            ['set alarm', 'set clock at 6', 'remind me at 9pm']])\n",
    "message_texts = None #'delete the value'\n",
    "\n",
    "#credentials_file = 'NewAgent-676cf2e40a3f.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create_intent(project_id, display_name, training_phrases_parts, message_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create intents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intent created: name: \"projects/newagent-fpxjnq/agent/intents/c2a01474-1e27-41a8-8709-861ad05dec7c\"\n",
      "display_name: \"delete\"\n",
      "priority: 500000\n",
      "messages {\n",
      "  text {\n",
      "  }\n",
      "}\n",
      "\n",
      "Intent created: name: \"projects/newagent-fpxjnq/agent/intents/c2687222-2a85-44ae-a192-0b5297863033\"\n",
      "display_name: \"transfer\"\n",
      "priority: 500000\n",
      "messages {\n",
      "  text {\n",
      "  }\n",
      "}\n",
      "\n",
      "Intent created: name: \"projects/newagent-fpxjnq/agent/intents/e4de3247-0e1e-42ea-8d3e-6a97e5da205f\"\n",
      "display_name: \"alarm\"\n",
      "priority: 500000\n",
      "messages {\n",
      "  text {\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "intents_client = dialogflow.IntentsClient()\n",
    "parent = intents_client.project_agent_path(project_id)\n",
    "\n",
    "for i in range(len(display_name)):\n",
    "    training_phrases = []\n",
    "    for training_phrases_part in training_phrases_parts[i]:\n",
    "        part = dialogflow.types.Intent.TrainingPhrase.Part(\n",
    "            text=training_phrases_part)\n",
    "        # Here we create a new training phrase for each provided part.\n",
    "        training_phrase = dialogflow.types.Intent.TrainingPhrase(parts=[part])\n",
    "        training_phrases.append(training_phrase)\n",
    "\n",
    "    text = dialogflow.types.Intent.Message.Text(text=message_texts)\n",
    "    message = dialogflow.types.Intent.Message(text=text)\n",
    "\n",
    "    intent = dialogflow.types.Intent(\n",
    "        display_name=display_name[i],\n",
    "        training_phrases=training_phrases,\n",
    "        messages=[message])\n",
    "\n",
    "    response = intents_client.create_intent(parent, intent)\n",
    "\n",
    "    print('Intent created: {}'.format(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# training the agent model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = dialogflow.AgentsClient()\n",
    "\n",
    "parent = client.project_path(project_id)\n",
    "\n",
    "response = client.train_agent(parent)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# get the intent ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['e4de3247-0e1e-42ea-8d3e-6a97e5da205f']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name = display_name[2]\n",
    "\n",
    "intents_client = dialogflow.IntentsClient()\n",
    "parent = intents_client.project_agent_path(project_id)\n",
    "\n",
    "intents = intents_client.list_intents(parent)\n",
    "\n",
    "intent_names = [\n",
    "    intent.name for intent in intents\n",
    "    if intent.display_name == name]\n",
    "\n",
    "intent_ids = [\n",
    "    intent_name.split('/')[-1] for intent_name\n",
    "    in intent_names]\n",
    "\n",
    "intent_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# detect the intent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query text: set alarm\n",
      "Detected intent: alarm\n",
      "Detected intent confidence: 1.0\n"
     ]
    }
   ],
   "source": [
    "DIALOGFLOW_PROJECT_ID = project_id\n",
    "DIALOGFLOW_LANGUAGE_CODE = 'en'\n",
    "SESSION_ID = '1'\n",
    "\n",
    "\n",
    "#text_to_be_analyzed = \"transfer money\"\n",
    "text_to_be_analyzed = \"set alarm\"\n",
    "\n",
    "\n",
    "session_client = dialogflow.SessionsClient()\n",
    "session = session_client.session_path(DIALOGFLOW_PROJECT_ID, SESSION_ID)\n",
    "\n",
    "text_input = dialogflow.types.TextInput(text=text_to_be_analyzed, language_code=DIALOGFLOW_LANGUAGE_CODE)\n",
    "query_input = dialogflow.types.QueryInput(text=text_input)\n",
    "\n",
    "try:\n",
    "    response = session_client.detect_intent(session=session, query_input=query_input)\n",
    "except InvalidArgument:\n",
    "    raise\n",
    "\n",
    "print(\"Query text:\", response.query_result.query_text)\n",
    "print(\"Detected intent:\", response.query_result.intent.display_name)\n",
    "print(\"Detected intent confidence:\", response.query_result.intent_detection_confidence)\n",
    "#print(\"Fulfillment text:\", response.query_result.fulfillment_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# delete all intents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['delete', 'transfer', 'alarm']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "insurance\n",
      "application_status\n",
      "rollover_401k\n",
      "user_name\n",
      "payday\n",
      "todo_list_update\n",
      "restaurant_reservation\n",
      "definition\n",
      "fun_fact\n",
      "pto_request\n",
      "reminder\n",
      "distance\n",
      "repeat\n",
      "travel_alert\n",
      "timer\n",
      "find_phone\n",
      "oil_change_how\n",
      "meaning_of_life\n",
      "change_user_name\n",
      "credit_score\n",
      "lost_luggage\n",
      "where_are_you_from\n",
      "maybe\n",
      "who_made_you\n",
      "freeze_account\n",
      "replacement_card_duration\n",
      "transactions\n",
      "pay_bill\n",
      "improve_credit_score\n",
      "tire_pressure\n",
      "balance\n",
      "pto_request_status\n",
      "spending_history\n",
      "confirm_reservation\n",
      "restaurant_suggestion\n",
      "travel_suggestion\n",
      "mpg\n",
      "transfer\n",
      "next_song\n",
      "apr\n",
      "change_language\n",
      "flip_coin\n",
      "oil_change_when\n",
      "time\n",
      "shopping_list_update\n",
      "translate\n",
      "new_card\n",
      "insurance_change\n",
      "nutrition_info\n",
      "flight_status\n",
      "yes\n",
      "timezone\n",
      "no\n",
      "what_can_i_ask_you\n",
      "rewards_balance\n",
      "change_speed\n"
     ]
    }
   ],
   "source": [
    "client = dialogflow.IntentsClient()\n",
    "\n",
    "parent = client.project_agent_path(project_id)\n",
    "\n",
    "intents = client.list_intents(parent)\n",
    "for i in intents:\n",
    "    print(i.display_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = dialogflow.IntentsClient()\n",
    "\n",
    "parent = client.project_agent_path(project_id)\n",
    "\n",
    "intents = client.list_intents(parent)\n",
    "\n",
    "response = client.batch_delete_intents(parent, intents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "intents = client.list_intents(parent)\n",
    "for i in intents:\n",
    "    print(i.display_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def detect_intent(project_id, text_to_be_analyzed):\n",
    "    DIALOGFLOW_PROJECT_ID = project_id\n",
    "    DIALOGFLOW_LANGUAGE_CODE = 'en'\n",
    "    SESSION_ID = '1'\n",
    "\n",
    "    #text_to_be_analyzed = \"transfer money\"\n",
    "    text_to_be_analyzed = text_to_be_analyzed #\"set alarm\"\n",
    "\n",
    "\n",
    "    session_client = dialogflow.SessionsClient()\n",
    "    session = session_client.session_path(DIALOGFLOW_PROJECT_ID, SESSION_ID)\n",
    "\n",
    "    text_input = dialogflow.types.TextInput(text=text_to_be_analyzed, language_code=DIALOGFLOW_LANGUAGE_CODE)\n",
    "    query_input = dialogflow.types.QueryInput(text=text_input)\n",
    "\n",
    "    try:\n",
    "        response = session_client.detect_intent(session=session, query_input=query_input)\n",
    "        return response.query_result.intent.display_name\n",
    "    \n",
    "    except: #InvalidArgument:\n",
    "        pass #raise\n",
    "        \n",
    "    '''\n",
    "    print(\"Query text:\", response.query_result.query_text)\n",
    "    print(\"Detected intent:\", response.query_result.intent.display_name)\n",
    "    print(\"Detected intent confidence:\", response.query_result.intent_detection_confidence)\n",
    "    #print(\"Fulfillment text:\", response.query_result.fulfillment_text)\n",
    "    '''\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "def eval_intent(project_id, test_data, write_file):  # input test data shoulb be in format .csv\n",
    "    \n",
    "    for i in range(len(test_data)):\n",
    "        if i % 150 == 1:\n",
    "            time.sleep(65)\n",
    "            print('Evaluating intent need to sleep for 65s!')\n",
    "\n",
    "        pred_intent = detect_intent(project_id, test_data.loc[i, 'Phrase'])\n",
    "        test_data.loc[i, 'Pred_intent'] = pred_intent\n",
    "        \n",
    "    test_data.dropna(axis=0, how='any', inplace=True)  \n",
    "    test_data.to_csv('./dialogflow_result/test_data_result.csv')\n",
    "    print('Evaluation is done!')\n",
    "    \n",
    "    \n",
    "    intent_set = list(set(list(test_data.Intent.unique()) + list(test_data.Pred_intent.unique())))\n",
    "    con_mat =  np.zeros([len(intent_set), len(intent_set)])  # True * Prediction\n",
    "\n",
    "    for i in range(len(test_data)):\n",
    "        con_mat[intent_set.index(test_data.loc[i, 'Intent']), intent_set.index(test_data.loc[i, 'Pred_intent'])] += 1\n",
    "        np.save(write_file+'/con_mat.npy', con_mat)\n",
    "    print('confusion matrix is done!')\n",
    "\n",
    "    \n",
    "    TP,TN,FP,FN,accuracy,precision,recall,f1 = 0,0,0,0,0,0,0,0\n",
    "    precision_sum, recall_sum, f1_sum = 0,0,0\n",
    "    d = 0\n",
    "    for i in range(len(con_mat)-1):\n",
    "        TP = con_mat[i][i]\n",
    "        FN = (np.sum(con_mat[i, :i]) + np.sum(con_mat[i, i+1:]))\n",
    "        FP = (np.sum(con_mat[:i, i]) + np.sum(con_mat[i+1:, i]))\n",
    "        TN = (np.sum(con_mat) - TP - FN - FP)\n",
    "\n",
    "        if  (TP + FP) != 0 and  (TP + FN) != 0:\n",
    "            d += 1\n",
    "            accuracy += TP / (TP + TN + FP + FN)\n",
    "\n",
    "            precision = TP / (TP + FP)\n",
    "            precision_sum += precision\n",
    "\n",
    "            recall = TP / (TP + FN)\n",
    "            recall_sum += recall\n",
    "\n",
    "            f1 = (2 * precision * recall / (precision + recall))\n",
    "            f1_sum += f1\n",
    "    print(accuracy,precision,recall,f1)\n",
    "\n",
    "    TP = con_mat[-1][-1]\n",
    "    FN = np.sum(con_mat[len(con_mat)-1, :len(con_mat)-1])\n",
    "    FP = np.sum(con_mat[:len(con_mat)-1, len(con_mat)-1])\n",
    "    TN = (np.sum(con_mat) - TP - FN - FP)\n",
    "\n",
    "    if  (TP + FP) != 0 and  (TP + FN) != 0:\n",
    "        d += 1\n",
    "        accuracy += TP / (TP + TN + FP + FN)\n",
    "\n",
    "        precision = TP / (TP + FP)\n",
    "        precision_sum += precision\n",
    "\n",
    "        recall = TP / (TP + FN)\n",
    "        recall_sum += recall\n",
    "\n",
    "        f1 = (2 * precision * recall / (precision + recall))\n",
    "        f1_sum += f1\n",
    "\n",
    "    print(accuracy, precision_sum/d, recall_sum/d, f1_sum/d)\n",
    "\n",
    "    overall_accuracy, overall_precision, overall_f1 = accuracy, precision_sum/d, f1_sum/d\n",
    "    \n",
    "    return overall_accuracy, overall_precision, overall_f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_id = 'newagent-fpxjnq'\n",
    "df_test = pd.read_csv('./data/df_test.csv', index_col=0)[:1000]\n",
    "eval_metrics = []\n",
    "sub_file_name = 'name'\n",
    "write_file = './dialogflow_result/sub_datasets/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating intent need to sleep for 65s!\n",
      "Evaluating intent need to sleep for 65s!\n",
      "Evaluating intent need to sleep for 65s!\n",
      "Evaluating intent need to sleep for 65s!\n",
      "Evaluating intent need to sleep for 65s!\n",
      "Evaluating intent need to sleep for 65s!\n",
      "Evaluating intent need to sleep for 65s!\n",
      "Evaluation is done!\n",
      "confusion matrix is done!\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (33,) (0,) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-8d342b6dd97a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# rasa NLU models evaluation part.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0moverall_accuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverall_precision\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverall_f1_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval_intent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproject_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrite_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0meval_metrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msub_file_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverall_accuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverall_precision\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverall_f1_score\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval_metrics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model_name'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'overall_accuracy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'overall_precision'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'overall_f1_score'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-30-50639d06a573>\u001b[0m in \u001b[0;36meval_intent\u001b[0;34m(project_id, test_data, write_file)\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0mTN\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcon_mat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcon_mat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0mFP\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcon_mat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcon_mat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m         \u001b[0mFN\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcon_mat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcon_mat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcon_mat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcon_mat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mTP\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mTN\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mFP\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0mTP\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mcon_mat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (33,) (0,) "
     ]
    }
   ],
   "source": [
    "# rasa NLU models evaluation part.\n",
    "overall_accuracy, overall_precision, overall_f1_score = eval_intent(project_id, df_test, write_file)\n",
    "eval_metrics.append([sub_file_name, overall_accuracy, overall_precision, overall_f1_score])\n",
    "\n",
    "result = pd.DataFrame(eval_metrics, columns=['model_name', 'overall_accuracy', 'overall_precision', 'overall_f1_score'])\n",
    "result.to_csv('./dialogflow_result/dialogflow_eval_result.csv')   # have to remove all previous result.csv at first\n",
    "print('The agent is evaluated successfully!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('finish!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90\n",
      "['change_user_name', nan, 'next_song', 'pto_request', 'income', 'order_checks', 'fun_fact', 'international_visa', 'who_made_you', 'vaccines', 'freeze_account', 'calculator', 'last_maintenance', 'are_you_a_bot', 'ingredient_substitution', 'change_ai_name', 'book_hotel', 'shopping_list_update', 'payday', 'definition', 'where_are_you_from', 'weather', 'credit_score', 'nutrition_info', 'direct_deposit', 'plug_type', 'make_call', 'accept_reservations', 'book_flight', 'current_location', 'change_language', 'maybe', 'pto_request_status', 'flip_coin', 'balance', 'next_holiday', 'yes', 'schedule_maintenance', 'improve_credit_score', 'meaning_of_life', 'what_is_your_name', 'distance', 'restaurant_reservation', 'mpg', 'uber', 'account_blocked', 'insurance', 'confirm_reservation', 'rollover_401k', 'international_fees', 'who_do_you_work_for', 'travel_notification', 'order', 'todo_list_update', 'timezone', 'flight_status', 'user_name', 'replacement_card_duration', 'routing', 'how_busy', 'time', 'what_are_your_hobbies', 'change_accent', 'cancel_reservation', 'application_status', 'tell_joke', 'card_declined', 'spelling', 'translate', 'transfer', 'min_payment', 'travel_suggestion', 'calendar_update', 'no', 'recipe', 'gas', 'travel_alert', 'find_phone', 'directions', 'timer', 'pto_balance', 'new_card', 'insurance_change', 'what_can_i_ask_you', 'todo_list', 'meal_suggestion', 'damaged_card', 'lost_luggage', 'how_old_are_you', 'oil_change_how']\n"
     ]
    }
   ],
   "source": [
    "test_data = pd.read_csv('./dialogflow_result/test_data_result.csv', index_col=0)\n",
    "\n",
    "intent_set = list(set(list(test_data.Intent.unique()) + list(test_data.Pred_intent.unique())))\n",
    "\n",
    "con_mat =  np.zeros([len(intent_set), len(intent_set)])  # True * Prediction\n",
    "print(len(intent_set))\n",
    "print(intent_set) \n",
    "\n",
    "for i in range(len(test_data)):\n",
    "    #print(test_data.loc[i, 'Intent'])\n",
    "    #print(test_data.loc[i, 'Pred_intent'])\n",
    "    con_mat[intent_set.index(test_data.loc[i, 'Intent']), intent_set.index(test_data.loc[i, 'Pred_intent'])] += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8060000000000005 0.8571428571428571 0.8 0.8275862068965518\n",
      "0.8360000000000005 0.9550415457856802 0.8392156862745095 0.8862033722288207\n",
      "34\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nfor i in range(len(con_mat)-1):\\n    TP += con_mat[i][i]\\n    FN += (np.sum(con_mat[i, :i]) + np.sum(con_mat[i, i+1:]))\\n    FP += (np.sum(con_mat[:i, i]) + np.sum(con_mat[i+1:, i]))\\nprint(TP,FN,FP)\\n\\n\\nTP += con_mat[-1][-1]\\nFN += np.sum(con_mat[len(con_mat)-1, :len(con_mat)-1])\\nFP += np.sum(con_mat[:len(con_mat)-1, len(con_mat)-1])\\nprint(TP,FN,FP)\\n\\naccuracy = TP / np.sum(con_mat)\\nprecision = TP / (TP + FP)\\nrecall = TP / (TP + FN)\\nf1 = 2 * precision * recall / (precision + recall)\\nprint(accuracy,precision,recall,f1,np.sum(con_mat))\\n'"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TP,TN,FP,FN,accuracy,precision,recall,f1 = 0,0,0,0,0,0,0,0\n",
    "precision_sum, recall_sum, f1_sum = 0,0,0\n",
    "\n",
    "d = 0\n",
    "for i in range(len(con_mat)-1):\n",
    "    TP = con_mat[i][i]\n",
    "    FN = (np.sum(con_mat[i, :i]) + np.sum(con_mat[i, i+1:]))\n",
    "    FP = (np.sum(con_mat[:i, i]) + np.sum(con_mat[i+1:, i]))\n",
    "    TN = (np.sum(con_mat) - TP - FN - FP)\n",
    "    #(np.sum(con_mat[:i, :i]) + np.sum(con_mat[i+1:, i+1:]) + np.sum(con_mat[:i, i+1:]) + np.sum(con_mat[i+1:, :i]) - TP - TN - FP)\n",
    "    if  (TP + FP) != 0 and  (TP + FN) != 0:\n",
    "        d += 1\n",
    "        accuracy += TP / (TP + TN + FP + FN)\n",
    "        \n",
    "        precision = TP / (TP + FP)\n",
    "        precision_sum += precision\n",
    "        \n",
    "        recall = TP / (TP + FN)\n",
    "        recall_sum += recall\n",
    "        \n",
    "        f1 = (2 * precision * recall / (precision + recall))\n",
    "        f1_sum += f1\n",
    "        \n",
    "print(accuracy,precision,recall,f1)\n",
    "\n",
    "\n",
    "TP = con_mat[-1][-1]\n",
    "FN = np.sum(con_mat[len(con_mat)-1, :len(con_mat)-1])\n",
    "FP = np.sum(con_mat[:len(con_mat)-1, len(con_mat)-1])\n",
    "TN = (np.sum(con_mat) - TP - FN - FP)\n",
    "#print(TP,FN,FP,TN)\n",
    "\n",
    "if  (TP + FP) != 0 and  (TP + FN) != 0:\n",
    "    d += 1\n",
    "    accuracy += TP / (TP + TN + FP + FN)\n",
    "    \n",
    "    precision = TP / (TP + FP)\n",
    "    precision_sum += precision\n",
    "\n",
    "    recall = TP / (TP + FN)\n",
    "    recall_sum += recall\n",
    "\n",
    "    f1 = (2 * precision * recall / (precision + recall))\n",
    "    f1_sum += f1\n",
    "    \n",
    "print(accuracy, precision_sum/d, recall_sum/d, f1_sum/d)\n",
    "\n",
    "print(d)\n",
    "\n",
    "'''\n",
    "for i in range(len(con_mat)-1):\n",
    "    TP += con_mat[i][i]\n",
    "    FN += (np.sum(con_mat[i, :i]) + np.sum(con_mat[i, i+1:]))\n",
    "    FP += (np.sum(con_mat[:i, i]) + np.sum(con_mat[i+1:, i]))\n",
    "print(TP,FN,FP)\n",
    "\n",
    "\n",
    "TP += con_mat[-1][-1]\n",
    "FN += np.sum(con_mat[len(con_mat)-1, :len(con_mat)-1])\n",
    "FP += np.sum(con_mat[:len(con_mat)-1, len(con_mat)-1])\n",
    "print(TP,FN,FP)\n",
    "\n",
    "accuracy = TP / np.sum(con_mat)\n",
    "precision = TP / (TP + FP)\n",
    "recall = TP / (TP + FN)\n",
    "f1 = 2 * precision * recall / (precision + recall)\n",
    "print(accuracy,precision,recall,f1,np.sum(con_mat))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:  0.836\n",
      "precision:  0.836\n",
      "f1:  0.836\n"
     ]
    }
   ],
   "source": [
    "print('accuracy: ', accuracy)\n",
    "print('precision: ', precision)\n",
    "print('f1: ', f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loop run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dialogflow_NLU\n",
    "\n",
    "import dialogflow_v2 as dialogflow\n",
    "import os\n",
    "\n",
    "credentials_file = 'newagent-fpxjnq-08c3138e5a20.json'  \n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = credentials_file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_id = 'newagent-fpxjnq'\n",
    "language_code = ''\n",
    "display_name = list(['delete', 'transfer', 'alarm'])\n",
    "training_phrases_parts = list([['delete the row please.', 'delete the column please.'],\n",
    "                            ['transfer 50 euro please', 'i want to make transfer', 'transfer money'],\n",
    "                            ['set alarm', 'set clock at 6', 'remind me at 9pm']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dialogflow_NLU.create_intent(project_id, display_name, training_phrases_parts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dialogflow_NLU.train_agent(project_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query text: transfer money\n",
      "Detected intent: transfer\n",
      "Detected intent confidence: 1.0\n",
      "Query text: set alarm\n",
      "Detected intent: alarm\n",
      "Detected intent confidence: 1.0\n"
     ]
    }
   ],
   "source": [
    "text_to_be_analyzed = ['transfer money', 'set alarm']\n",
    "\n",
    "for text in text_to_be_analyzed:\n",
    "    dialogflow_NLU.detect_intent(project_id, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intent before delete:  0\n",
      "--------------------\n",
      "Intent after delete:  0\n"
     ]
    }
   ],
   "source": [
    "dialogflow_NLU.delete_all_intents(project_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data_file = '/users/xinsun/Downloads/oos-eval-master/data/data_full.json'\n",
    "df = pd.read_json(data_file, typ='series')\n",
    "\n",
    "df_oos_train = pd.DataFrame(df['oos_train'], columns=['Phrase', 'Intent'])\n",
    "df_oos_test = pd.DataFrame(df['oos_test'], columns=['Phrase', 'Intent'])\n",
    "df_oos_val = pd.DataFrame(df['oos_val'], columns=['Phrase', 'Intent'])\n",
    "df_train = pd.DataFrame(df['train'], columns=['Phrase', 'Intent'])\n",
    "df_test = pd.DataFrame(df['test'], columns=['Phrase', 'Intent'])\n",
    "df_val = pd.DataFrame(df['val'], columns=['Phrase', 'Intent'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "intents_set = list(df_train['Intent'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrases_set = [list(df_train[df_train.Intent==intent].Phrase.values) for intent in intents_set]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'translate'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intents_set[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['what expression would i use to say i love you if i were an italian',\n",
       " \"can you tell me how to say 'i do not speak much spanish', in spanish\"]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phrases_set[0][:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_id = 'newagent-fpxjnq'\n",
    "language_code = ''\n",
    "display_name = intents_set[:30]\n",
    "training_phrases_parts = phrases_set[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intent before delete:  0\n",
      "--------------------\n",
      "Intent after delete:  0\n"
     ]
    }
   ],
   "source": [
    "dialogflow_NLU.delete_all_intents(project_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dialogflow_NLU.create_intent(project_id, display_name, training_phrases_parts)\n",
    "\n",
    "dialogflow_NLU.train_agent(project_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query text: transfer money\n",
      "Detected intent: transfer\n",
      "Detected intent confidence: 0.8908178806304932\n",
      "Query text: how can i say in english\n",
      "Detected intent: translate\n",
      "Detected intent confidence: 0.8272097110748291\n",
      "Query text: set the alarm\n",
      "Detected intent: \n",
      "Detected intent confidence: 0.0\n"
     ]
    }
   ],
   "source": [
    "text_to_be_analyzed = ['transfer money', 'how can i say in english', 'set the alarm']\n",
    "\n",
    "for i in range(len(text_to_be_analyzed)):\n",
    "    dialogflow_NLU.detect_intent(project_id, text_to_be_analyzed[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intent before delete:  50\n",
      "--------------------\n",
      "Intent after delete:  0\n"
     ]
    }
   ],
   "source": [
    "dialogflow_NLU.delete_all_intents(project_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the sub-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data_file = './data/data_full.json'\n",
    "\n",
    "# get the test dataset\n",
    "df = pd.read_json(data_file, typ='series')\n",
    "df_test = pd.DataFrame(df['test'], columns=['Phrase', 'Intent'])\n",
    "df_oov_test = pd.DataFrame(df['oos_test'], columns=['Phrase', 'Intent'])\n",
    "df_test = pd.concat([df_test, df_oov_test]).reset_index(drop=True)\n",
    "\n",
    "write_test_file = './data/df_test.csv'\n",
    "\n",
    "df_test[['Phrase', 'Intent']].to_csv(write_test_file) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(write_test_file, index_col=0)\n",
    "len(test.Intent.unique())\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3.7.3 64-bit",
   "language": "python",
   "name": "python37364bit8d437ac45ab1400083d7026870debc43"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
